{"cells":[{"metadata":{"id":"fxs9AxSut8tU"},"cell_type":"markdown","source":"# Light GBM"},{"metadata":{"id":"21meSRURPZKO"},"cell_type":"markdown","source":"Notebook used as example: https://www.kaggle.com/robertburbidge/lightgbm-poisson-w-scaled-pinball-loss"},{"metadata":{"id":"VbIniaCyuKW0"},"cell_type":"markdown","source":"For this competition we used Light GBM, a gradient boosting framework that uses tree based learning algorithm."},{"metadata":{"id":"Xdy-e7TIXpLp","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nfrom scipy.sparse import csr_matrix\n\nimport os ","execution_count":null,"outputs":[]},{"metadata":{"id":"CyFUEc4quZo9"},"cell_type":"markdown","source":"Function for memory reduction, this speeds up the notebook. Also convenient for preventing the uses of all available RAM."},{"metadata":{"id":"z5TEnUzFcrD0","trusted":false},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"id":"D8L6jK-yu-34"},"cell_type":"markdown","source":"Load data provided by the M5 Forecasting competition and reduce the memory."},{"metadata":{"id":"IYXmPJKuXy1o","outputId":"58cc81d1-68ea-4f03-d5ac-225040423357","trusted":false},"cell_type":"code","source":"calendar = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv'))\nsell_prices = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv'))\nsales_train_eval = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv'))\nsubmission = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"id":"N0QhnNI-xvF1"},"cell_type":"markdown","source":"The function encode_categorical, encodes the non NaN strings in the specified columns to integers. "},{"metadata":{"id":"yCt-_5CprgP3","trusted":false},"cell_type":"code","source":"def encode_categorical(df, cols):\n  for col in cols:\n    le = LabelEncoder()\n    not_null = df[col][df[col].notnull()]\n    df[col] = pd.Series(le.fit_transform(not_null), index = not_null.index)\n  return df","execution_count":null,"outputs":[]},{"metadata":{"id":"saEeJAmjx9RX"},"cell_type":"markdown","source":"Data preprocessing before training."},{"metadata":{"id":"GU7CDdyirxF1","outputId":"b408418e-734a-484e-8cda-e842d988617b","trusted":false},"cell_type":"code","source":"NUM_ITEMS = sales_train_eval.shape[0]\nDAYS_PRED = submission.shape[1] - 1 \n\n# Encode the specified columns\ncalendar = encode_categorical(calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]).pipe(reduce_mem_usage)\nsales_train_eval = encode_categorical(sales_train_eval, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]).pipe(reduce_mem_usage)\nsell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)\n\nnrows = 365 * 1 * NUM_ITEMS\n\n# Reshape the dataframe sales_train_eval to a dataframe with the existing columns id, item_id, dept_id, cat_id, store_it and state_id \n# and in addition the new columns day and demand instead of a column for each day\nsales_train_eval = pd.melt(sales_train_eval, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\nsales_train_eval = sales_train_eval.iloc[-nrows:,:]\n\n# Add rows for forecasting\nforecast_submission = submission\n\nvalidation_rows = [row for row in forecast_submission['id'] if 'validation' in row]\nevaluation_rows = [row for row in forecast_submission['id'] if 'evaluation' in row]\n\nvalidation = forecast_submission[forecast_submission['id'].isin(validation_rows)]\nevaluation = forecast_submission[forecast_submission['id'].isin(evaluation_rows)]\n\nvalidation.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)] \nevaluation.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)] \n\nproduct = sales_train_eval[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n\nvalidation = validation.merge(product, how = 'left', on = 'id')\nevaluation = evaluation.merge(product, how = 'left', on = 'id')\n\nvalidation = pd.melt(validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\nevaluation = pd.melt(evaluation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n\nsales_train_eval['part'] = 'train'\nvalidation['part'] = 'test1'\nevaluation['part'] = 'test2'\n\n# Add validation and evaluation rows to the sales_train_eval dataframe\ndata = pd.concat([sales_train_eval, validation, evaluation], axis = 0)\ndata = reduce_mem_usage(data)\n\ndel validation, evaluation, sales_train_eval, forecast_submission\ngc.collect()\n\ncalendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndata.drop(['d', 'day'], inplace = True, axis = 1)\n\ndel  calendar\ngc.collect()\n\ndata = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n\ndel  sell_prices\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"JAaljE493-F0"},"cell_type":"markdown","source":"Select train and test data."},{"metadata":{"id":"o21vsvKSZDV6","outputId":"df3f7054-6aa4-4ced-c004-406e2149a587","trusted":false},"cell_type":"code","source":"data = reduce_mem_usage(data)\n\nx_train = data[data['date'] <= '2016-05-22']\ny_train = x_train['demand']\ntest = data[data['date'] > '2016-05-22']","execution_count":null,"outputs":[]},{"metadata":{"id":"mJhwSEHM4E1U"},"cell_type":"markdown","source":"Train with LGBM and WRMSSE."},{"metadata":{"id":"0dNGowxUuB7e","outputId":"0a77cb49-430b-4a56-a8b8-c7c4b13ffb7c","trusted":false},"cell_type":"code","source":"features = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \n            \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\",\n            \"snap_CA\", \"snap_TX\", \"snap_WI\", \"sell_price\"]\n\nweight_mat = np.c_[np.identity(NUM_ITEMS).astype(np.int8), np.ones([NUM_ITEMS, 1]).astype(np.int8),  \n                   pd.get_dummies(product.state_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.cat_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.dept_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.item_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str), drop_first=False).astype('int8').values\n].T\n\nweight_mat_csr = csr_matrix(weight_mat)\ndel weight_mat\ngc.collect()\n\ndef weight_calc(data):\n    sales_train_eval = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n    \n    day = ['d_' + str(i + 1) for i in range(1913)]\n    \n    sales_train_eval = weight_mat_csr * sales_train_eval[day].values\n    \n    df_temp = ((sales_train_eval > 0) * np.tile(np.arange(1, 1914), (weight_mat_csr.shape[0], 1)))\n\n    start_no = np.min(np.where(df_temp == 0, 9999, df_temp), axis=1) - 1\n    \n    weight1 = np.sum((np.diff(sales_train_eval, axis=1) ** 2), axis=1) / (1913 - start_no)\n    \n    df_temp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    df_temp['amount'] = df_temp['demand'] * df_temp['sell_price']\n    df_temp = df_temp.groupby(['id'])['amount'].apply(np.sum).values\n   \n    weight2 = weight_mat_csr * df_temp\n    weight2 = weight2 / np.sum(weight2)\n    \n    del sales_train_eval\n    gc.collect()\n    \n    return weight1, weight2\n\nweight1, weight2 = weight_calc(data)\n\ndef wrmsse(preds, data):\n    y_true = np.array(data.get_label())\n    \n    num_col = len(y_true) // NUM_ITEMS\n\n    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n    \n    x_name = ['pred_' + str(i) for i in range(num_col)]\n    x_name2 = [\"act_\" + str(i) for i in range(num_col)]\n    \n    train = np.array(weight_mat_csr * np.c_[reshaped_preds, reshaped_true])\n    \n    score = np.sum(np.sqrt(np.mean(np.square(train[:, :num_col] - train[:, num_col:]), axis=1) / weight1) * weight2)\n    \n    return 'wrmsse', score, False\n\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'custom',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10,\n    'colsample_bytree': 0.75\n}\n\ntrain_data = lgb.Dataset(x_train[features], y_train)\n\nmodel = lgb.train(params, train_data, num_boost_round=2500, early_stopping_rounds=50, valid_sets=[train_data], verbose_eval=100, feval=wrmsse)\n\ny_test_pred = model.predict(test[features], num_iteration=model.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"id":"CidoXAXJ5cD8"},"cell_type":"markdown","source":"Concatenate the evaluations predictions to the validation real data for the submission file."},{"metadata":{"id":"qAfLSvdTIyOh","outputId":"6a662a20-704d-4d81-87e4-cdde4022bf7f","trusted":false},"cell_type":"code","source":"test['demand'] = y_test_pred\n\n# Get the predictions for the evaluation rows for submission\npredictions = test[['id', 'date', 'demand']]\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n# Get the real data for the validation rows for submission \neval_data = data[['id', 'date', 'demand']]\neval_data = eval_data[(eval_data['date'] > '2016-04-24') & (eval_data['date'] <= '2016-05-22')]\neval_data = pd.pivot(eval_data, index='id', columns='date', values='demand').reset_index()\neval_data.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\neval_data = eval_data[~eval_data['id'].str.contains(\"validation\")]\neval_data['id'] = eval_data['id'].str.replace(\"_evaluation\", \"_validation\")\n\npredictions = predictions.append(eval_data, ignore_index=True)\npredictions.to_csv('../output/submission-accuracy/submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}